{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3e0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea90a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential, clone_model, model_from_json\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Activation, Layer,LeakyReLU\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "import gym\n",
    "import time\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "from datetime import datetime\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59180a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [-2. , -1.6, -1.2, -0.8, -0.4,  0. ,  0.4,  0.8,  1.2,  1.6,  2. ]\n",
    "def epsilon_greedy(model, state, eps):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - max(eps, 0.05)): # Burada mutlaka keşfetmeyi sağlama amacıyla yapıldı\n",
    "        max_index = np.argmax(model.predict(state,verbose=0))\n",
    "        return np.array([action_space[max_index]]), max_index\n",
    "    else:\n",
    "        random_index = np.random.choice(np.arange(len(action_space)))\n",
    "        return np.array([action_space[random_index]]), random_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de317ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"Pendulum-v1\")\n",
    "#\n",
    "#state = env.reset()\n",
    "#\n",
    "#state = np.reshape(state , [1,state_size])\n",
    "#epsilon_greedy(DQNetworkPendulum, state, 1)\n",
    "#a,_ = epsilon_greedy(DQNetworkPendulum, state, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd081a2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aca82c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLog\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLog\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m memory \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(\"Log\"):\n",
    "    os.mkdir(\"Log\")\n",
    "\n",
    "memory = deque(maxlen=10000) \n",
    "min_experience = 1000\n",
    "batch_size = 64\n",
    "np.random.seed(124)\n",
    "\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "action_space = [-2. , -1.6, -1.2, -0.8, -0.4,  0. ,  0.4,  0.8,  1.2,  1.6,  2. ]\n",
    "\n",
    "state_size = len(env.observation_space.sample())\n",
    "action_size = len(action_space)\n",
    "\n",
    "\n",
    "#Prediction Model\n",
    "DQNetworkPendulum = Sequential(name = \"DQNetworkPendulum\")\n",
    "DQNetworkPendulum.add(Dense(64, \n",
    "                            activation = LeakyReLU(alpha = 0.01),\n",
    "                            input_shape = (state_size, ),\n",
    "                            kernel_initializer = 'he_uniform'))\n",
    "DQNetworkPendulum.add(Dense(32, \n",
    "                            activation = LeakyReLU(alpha = 0.01), \n",
    "                            kernel_initializer = 'he_uniform'))\n",
    "DQNetworkPendulum.add(Dense(action_size,\n",
    "                            activation = 'linear' ))\n",
    "\n",
    "DQN_optimizer = Adam(learning_rate = 0.01)\n",
    "\n",
    "DQNetworkPendulum.compile(optimizer = DQN_optimizer, \n",
    "                          loss = 'mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "#Target Model\n",
    "TargetNetworkPendulum = Sequential(name = \"DQNetworkPendulum\")\n",
    "TargetNetworkPendulum.add(Dense(64, \n",
    "                                activation = LeakyReLU(alpha = 0.01),\n",
    "                                input_shape = (state_size, ),\n",
    "                                kernel_initializer = 'he_uniform'))\n",
    "TargetNetworkPendulum.add(Dense(32, \n",
    "                                activation = LeakyReLU(alpha = 0.01), \n",
    "                                kernel_initializer = 'he_uniform'))\n",
    "TargetNetworkPendulum.add(Dense(action_size,\n",
    "                                activation = 'linear' ))\n",
    "\n",
    "TargetN_optimizer = Adam(learning_rate = 0.01)\n",
    "\n",
    "TargetNetworkPendulum.compile(optimizer = TargetN_optimizer, \n",
    "                              loss = 'mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TargetNetworkPendulum.set_weights(DQNetworkPendulum.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c5cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, score: -1473.8373063696692\n",
      "elapse time of 0th episode is: 0:00:00.060468\n",
      "episode: 1, score: -1485.6425413490954\n",
      "elapse time of 1th episode is: 0:00:00.984953\n",
      "episode: 2, score: -1015.0401255920873\n",
      "elapse time of 2th episode is: 0:00:00.846829\n",
      "episode: 3, score: -969.0088101028051\n",
      "elapse time of 3th episode is: 0:00:01.173495\n",
      "episode: 4, score: -1324.9759848546996\n",
      "elapse time of 4th episode is: 0:00:02.305984\n",
      "episode: 5, score: -972.0450404373522\n",
      "elapse time of 5th episode is: 0:00:39.210470\n",
      "episode: 6, score: -900.966755696252\n",
      "elapse time of 6th episode is: 0:00:35.246241\n",
      "episode: 7, score: -1225.5120340677245\n",
      "elapse time of 7th episode is: 0:00:36.231302\n",
      "episode: 8, score: -999.9290036708284\n",
      "elapse time of 8th episode is: 0:00:33.845198\n",
      "episode: 9, score: -1255.2194850032934\n",
      "TargetNetwork equalized to DQNetworkPendulum at 9 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 9th episode is: 0:00:34.899362\n",
      "episode: 10, score: -873.9225468298287\n",
      "elapse time of 10th episode is: 0:00:34.728609\n",
      "episode: 11, score: -1013.5498939547722\n",
      "elapse time of 11th episode is: 0:00:46.054251\n",
      "episode: 12, score: -1186.3146814543006\n",
      "elapse time of 12th episode is: 0:00:48.285860\n",
      "episode: 13, score: -1030.7287689401344\n",
      "elapse time of 13th episode is: 0:00:35.495651\n",
      "episode: 14, score: -1153.7924975163949\n",
      "elapse time of 14th episode is: 0:00:47.234528\n",
      "episode: 15, score: -1078.5516123301504\n",
      "elapse time of 15th episode is: 0:00:44.790701\n",
      "episode: 16, score: -1095.180810285963\n",
      "elapse time of 16th episode is: 0:00:39.790839\n",
      "episode: 17, score: -1243.7726082202432\n",
      "elapse time of 17th episode is: 0:00:42.116457\n",
      "episode: 18, score: -1359.565978323231\n",
      "elapse time of 18th episode is: 0:00:39.539090\n",
      "episode: 19, score: -1037.1553682351664\n",
      "TargetNetwork equalized to DQNetworkPendulum at 19 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode19/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode19/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 19th episode is: 0:00:41.194098\n",
      "episode: 20, score: -1354.827461888709\n",
      "elapse time of 20th episode is: 0:00:39.927200\n",
      "episode: 21, score: -1341.7162499633637\n",
      "elapse time of 21th episode is: 0:00:40.576689\n",
      "episode: 22, score: -1265.731513840124\n",
      "elapse time of 22th episode is: 0:00:41.163312\n",
      "episode: 23, score: -1087.7145681771779\n",
      "elapse time of 23th episode is: 0:00:36.824516\n",
      "episode: 24, score: -1225.265615285544\n",
      "elapse time of 24th episode is: 0:00:37.444012\n",
      "episode: 25, score: -1472.7969804813565\n",
      "elapse time of 25th episode is: 0:00:41.875711\n",
      "episode: 26, score: -869.410295397813\n",
      "elapse time of 26th episode is: 0:00:37.188982\n",
      "episode: 27, score: -1270.9521177166062\n",
      "elapse time of 27th episode is: 0:00:40.553809\n",
      "episode: 28, score: -1395.5000441594962\n",
      "elapse time of 28th episode is: 0:00:59.521987\n",
      "episode: 29, score: -1258.560839620274\n",
      "TargetNetwork equalized to DQNetworkPendulum at 29 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode29/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode29/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 29th episode is: 0:01:06.933707\n",
      "episode: 30, score: -1315.997676979981\n",
      "elapse time of 30th episode is: 0:01:01.410955\n",
      "episode: 31, score: -871.9499301755434\n",
      "elapse time of 31th episode is: 0:01:00.251242\n",
      "episode: 32, score: -1354.0971594107327\n",
      "elapse time of 32th episode is: 0:01:00.603699\n",
      "episode: 33, score: -1605.9619283445923\n",
      "elapse time of 33th episode is: 0:00:59.926237\n",
      "episode: 34, score: -1353.4901778991139\n",
      "elapse time of 34th episode is: 0:01:00.251966\n",
      "episode: 35, score: -1475.7556749102687\n",
      "elapse time of 35th episode is: 0:01:01.548514\n",
      "episode: 36, score: -1544.928976919407\n",
      "elapse time of 36th episode is: 0:01:02.274848\n",
      "episode: 37, score: -1258.419758557436\n",
      "elapse time of 37th episode is: 0:01:01.162564\n",
      "episode: 38, score: -1407.189094360655\n",
      "elapse time of 38th episode is: 0:01:09.575554\n",
      "episode: 39, score: -1320.2908811452617\n",
      "TargetNetwork equalized to DQNetworkPendulum at 39 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode39/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode39/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 39th episode is: 0:01:05.360519\n",
      "episode: 40, score: -1420.624781809526\n",
      "elapse time of 40th episode is: 0:01:01.174685\n",
      "episode: 41, score: -1494.6260679656211\n",
      "elapse time of 41th episode is: 0:01:02.108621\n",
      "episode: 42, score: -1232.2028394859208\n",
      "elapse time of 42th episode is: 0:01:04.674121\n",
      "episode: 43, score: -1434.5092916509618\n",
      "elapse time of 43th episode is: 0:01:09.575090\n",
      "episode: 44, score: -1145.3422534741023\n",
      "elapse time of 44th episode is: 0:00:55.016601\n",
      "episode: 45, score: -1011.0242900700716\n",
      "elapse time of 45th episode is: 0:00:45.146428\n",
      "episode: 46, score: -1280.6271765553631\n",
      "elapse time of 46th episode is: 0:00:43.789500\n",
      "episode: 47, score: -1214.6322796684149\n",
      "elapse time of 47th episode is: 0:01:06.002518\n",
      "episode: 48, score: -1218.0014418014298\n",
      "elapse time of 48th episode is: 0:01:02.789372\n",
      "episode: 49, score: -1475.3914595652577\n",
      "TargetNetwork equalized to DQNetworkPendulum at 49 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode49/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode49/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 49th episode is: 0:01:04.527985\n",
      "episode: 50, score: -1450.8481245117523\n",
      "elapse time of 50th episode is: 0:01:06.142923\n",
      "episode: 51, score: -1370.280995706022\n",
      "elapse time of 51th episode is: 0:01:02.811162\n",
      "episode: 52, score: -1292.234181516375\n",
      "elapse time of 52th episode is: 0:01:01.975249\n",
      "episode: 53, score: -1160.3376434084032\n",
      "elapse time of 53th episode is: 0:01:11.664659\n",
      "episode: 54, score: -1157.6054637961677\n",
      "elapse time of 54th episode is: 0:01:06.443909\n",
      "episode: 55, score: -1394.9382513776839\n",
      "elapse time of 55th episode is: 0:01:04.431324\n",
      "episode: 56, score: -1223.2271196105876\n",
      "elapse time of 56th episode is: 0:01:14.665147\n",
      "episode: 57, score: -1042.1160931224374\n",
      "elapse time of 57th episode is: 0:01:01.270159\n",
      "episode: 58, score: -1196.883210986148\n",
      "elapse time of 58th episode is: 0:01:45.693619\n",
      "episode: 59, score: -1138.9055069961216\n",
      "TargetNetwork equalized to DQNetworkPendulum at 59 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode59/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode59/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 59th episode is: 0:01:12.961431\n",
      "episode: 60, score: -701.046335428914\n",
      "elapse time of 60th episode is: 0:01:09.041747\n",
      "episode: 61, score: -1090.7441704416626\n",
      "elapse time of 61th episode is: 0:01:11.250277\n",
      "episode: 62, score: -1140.9588543589055\n",
      "elapse time of 62th episode is: 0:01:13.119339\n",
      "episode: 63, score: -1292.5913469825389\n",
      "elapse time of 63th episode is: 0:01:10.489963\n",
      "episode: 64, score: -872.5848101878134\n",
      "elapse time of 64th episode is: 0:01:10.485608\n",
      "episode: 65, score: -1032.0977242119814\n",
      "elapse time of 65th episode is: 0:01:06.871265\n",
      "episode: 66, score: -911.4287057762883\n",
      "elapse time of 66th episode is: 0:01:10.350947\n",
      "episode: 67, score: -1024.3986157919974\n",
      "elapse time of 67th episode is: 0:01:15.745771\n",
      "episode: 68, score: -1381.1505789479793\n",
      "elapse time of 68th episode is: 0:01:09.984906\n",
      "episode: 69, score: -1081.7866771463662\n",
      "TargetNetwork equalized to DQNetworkPendulum at 69 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode69/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode69/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 69th episode is: 0:01:12.469144\n",
      "episode: 70, score: -780.5037188527734\n",
      "elapse time of 70th episode is: 0:01:11.592578\n",
      "episode: 71, score: -772.0387409987774\n",
      "elapse time of 71th episode is: 0:01:11.161118\n",
      "episode: 72, score: -931.7790794720388\n",
      "elapse time of 72th episode is: 0:01:11.375451\n",
      "episode: 73, score: -1115.2944108740833\n",
      "elapse time of 73th episode is: 0:01:10.567712\n",
      "episode: 74, score: -843.0471472902515\n",
      "elapse time of 74th episode is: 0:01:15.561676\n",
      "episode: 75, score: -1088.3852664868418\n",
      "elapse time of 75th episode is: 0:01:10.528171\n",
      "episode: 76, score: -531.643505316474\n",
      "elapse time of 76th episode is: 0:01:14.214731\n",
      "episode: 77, score: -957.4476865880937\n",
      "elapse time of 77th episode is: 0:01:15.106495\n",
      "episode: 78, score: -1096.2742694623707\n",
      "elapse time of 78th episode is: 0:01:07.307522\n",
      "episode: 79, score: -984.9359367282259\n",
      "TargetNetwork equalized to DQNetworkPendulum at 79 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode79/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode79/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 79th episode is: 0:01:08.392072\n",
      "episode: 80, score: -647.2569298094634\n",
      "elapse time of 80th episode is: 0:01:06.809811\n",
      "episode: 81, score: -577.035241092883\n",
      "elapse time of 81th episode is: 0:01:07.289652\n",
      "episode: 82, score: -902.7964703255932\n",
      "elapse time of 82th episode is: 0:01:16.941289\n",
      "episode: 83, score: -390.87603367331434\n",
      "elapse time of 83th episode is: 0:01:31.578634\n",
      "episode: 84, score: -1064.291341343196\n",
      "elapse time of 84th episode is: 0:01:11.109036\n",
      "episode: 85, score: -394.416136352777\n",
      "elapse time of 85th episode is: 0:01:07.629259\n",
      "episode: 86, score: -904.8028288445106\n",
      "elapse time of 86th episode is: 0:01:06.831485\n",
      "episode: 87, score: -1063.8810273698127\n",
      "elapse time of 87th episode is: 0:01:09.011757\n",
      "episode: 88, score: -886.7111468194961\n",
      "elapse time of 88th episode is: 0:01:08.737081\n",
      "episode: 89, score: -972.372436154978\n",
      "TargetNetwork equalized to DQNetworkPendulum at 89 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode89/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode89/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 89th episode is: 0:01:09.979446\n",
      "episode: 90, score: -1050.9899357073832\n",
      "elapse time of 90th episode is: 0:01:06.273148\n",
      "episode: 91, score: -1039.4043414358744\n",
      "elapse time of 91th episode is: 0:01:08.219379\n",
      "episode: 92, score: -398.21553089549343\n",
      "elapse time of 92th episode is: 0:01:05.652797\n",
      "episode: 93, score: -393.10275673678177\n",
      "elapse time of 93th episode is: 0:01:05.475665\n",
      "episode: 94, score: -937.929930346639\n",
      "elapse time of 94th episode is: 0:01:18.591573\n",
      "episode: 95, score: -807.3291774371068\n",
      "elapse time of 95th episode is: 0:01:16.205459\n",
      "episode: 96, score: -582.9314471907497\n",
      "elapse time of 96th episode is: 0:00:51.316929\n",
      "episode: 97, score: -553.1851798130643\n",
      "elapse time of 97th episode is: 0:01:03.112756\n",
      "episode: 98, score: -642.9225537158226\n",
      "elapse time of 98th episode is: 0:00:55.958664\n",
      "episode: 99, score: -716.1027785749225\n",
      "TargetNetwork equalized to DQNetworkPendulum at 99 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode99/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode99/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 99th episode is: 0:00:54.448437\n",
      "episode: 100, score: -768.4434755751904\n",
      "elapse time of 100th episode is: 0:00:50.798592\n",
      "episode: 101, score: -1023.1671125025674\n",
      "elapse time of 101th episode is: 0:00:50.287260\n",
      "episode: 102, score: -773.4550718062934\n",
      "elapse time of 102th episode is: 0:00:49.124184\n",
      "episode: 103, score: -1239.9546020917273\n",
      "elapse time of 103th episode is: 0:00:49.550529\n",
      "episode: 104, score: -657.1254259385281\n",
      "elapse time of 104th episode is: 0:00:48.667176\n",
      "episode: 105, score: -1012.1175948040471\n",
      "elapse time of 105th episode is: 0:00:49.689616\n",
      "episode: 106, score: -408.5730054960843\n",
      "elapse time of 106th episode is: 0:00:49.597598\n",
      "episode: 107, score: -643.1531084880424\n",
      "elapse time of 107th episode is: 0:00:48.706587\n",
      "episode: 108, score: -598.923871781208\n",
      "elapse time of 108th episode is: 0:00:51.607182\n",
      "episode: 109, score: -968.1345645931095\n",
      "TargetNetwork equalized to DQNetworkPendulum at 109 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode109/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode109/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 109th episode is: 0:00:51.797799\n",
      "episode: 110, score: -263.322424315666\n",
      "elapse time of 110th episode is: 0:00:49.875801\n",
      "episode: 111, score: -257.72891360685094\n",
      "elapse time of 111th episode is: 0:00:49.270494\n",
      "episode: 112, score: -520.8369342753324\n",
      "elapse time of 112th episode is: 0:00:49.736613\n",
      "episode: 113, score: -648.4888304385156\n",
      "elapse time of 113th episode is: 0:00:49.628660\n",
      "episode: 114, score: -259.30783744115297\n",
      "elapse time of 114th episode is: 0:00:49.073155\n",
      "episode: 115, score: -511.8581153633335\n",
      "elapse time of 115th episode is: 0:00:49.757461\n",
      "episode: 116, score: -366.75546630895855\n",
      "elapse time of 116th episode is: 0:00:49.048636\n",
      "episode: 117, score: -636.7595650508011\n",
      "elapse time of 117th episode is: 0:00:48.582596\n",
      "episode: 118, score: -432.7261552732863\n",
      "elapse time of 118th episode is: 0:00:49.353879\n",
      "episode: 119, score: -678.1745473419675\n",
      "TargetNetwork equalized to DQNetworkPendulum at 119 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode119/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode119/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 119th episode is: 0:00:49.008956\n",
      "episode: 120, score: -813.3285253721492\n",
      "elapse time of 120th episode is: 0:00:49.823452\n",
      "episode: 121, score: -254.26986985662728\n",
      "elapse time of 121th episode is: 0:00:49.130021\n",
      "episode: 122, score: -135.6349420956115\n",
      "elapse time of 122th episode is: 0:00:49.332981\n",
      "episode: 123, score: -138.22731269976458\n",
      "elapse time of 123th episode is: 0:00:48.959808\n",
      "episode: 124, score: -395.8718305630422\n",
      "elapse time of 124th episode is: 0:00:49.243213\n",
      "episode: 125, score: -261.18346489752327\n",
      "elapse time of 125th episode is: 0:00:48.915646\n",
      "episode: 126, score: -268.8728611242667\n",
      "elapse time of 126th episode is: 0:00:50.694738\n",
      "episode: 127, score: -8.773730371577175\n",
      "elapse time of 127th episode is: 0:00:50.422644\n",
      "episode: 128, score: -9.823835454394839\n",
      "elapse time of 128th episode is: 0:00:49.640946\n",
      "episode: 129, score: -262.5839810027783\n",
      "TargetNetwork equalized to DQNetworkPendulum at 129 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode129/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode129/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 129th episode is: 0:00:50.367700\n",
      "episode: 130, score: -378.16663277551453\n",
      "elapse time of 130th episode is: 0:00:49.083133\n",
      "episode: 131, score: -257.80120620426726\n",
      "elapse time of 131th episode is: 0:00:50.166062\n",
      "episode: 132, score: -260.0615512019102\n",
      "elapse time of 132th episode is: 0:00:49.924670\n",
      "episode: 133, score: -399.3979894148326\n",
      "elapse time of 133th episode is: 0:00:50.108726\n",
      "episode: 134, score: -131.70633102499966\n",
      "elapse time of 134th episode is: 0:01:15.342803\n",
      "episode: 135, score: -600.8751190648736\n",
      "elapse time of 135th episode is: 0:00:50.124077\n",
      "episode: 136, score: -6.53641754683514\n",
      "elapse time of 136th episode is: 0:00:52.332266\n",
      "episode: 137, score: -257.8043306202198\n",
      "elapse time of 137th episode is: 0:00:50.916680\n",
      "episode: 138, score: -261.6170669912876\n",
      "elapse time of 138th episode is: 0:00:50.400834\n",
      "episode: 139, score: -517.2233454253363\n",
      "TargetNetwork equalized to DQNetworkPendulum at 139 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode139/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode139/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 139th episode is: 0:00:50.711739\n",
      "episode: 140, score: -536.7691792569859\n",
      "elapse time of 140th episode is: 0:00:49.840984\n",
      "episode: 141, score: -377.17822291255266\n",
      "elapse time of 141th episode is: 0:00:50.002960\n",
      "episode: 142, score: -505.6335067718524\n",
      "elapse time of 142th episode is: 0:00:49.729344\n",
      "episode: 143, score: -553.7348355817086\n",
      "elapse time of 143th episode is: 0:00:50.549197\n",
      "episode: 144, score: -131.63215134246315\n",
      "elapse time of 144th episode is: 0:00:50.719649\n",
      "episode: 145, score: -260.7239447255022\n",
      "elapse time of 145th episode is: 0:00:49.091757\n",
      "episode: 146, score: -258.21467305960215\n",
      "elapse time of 146th episode is: 0:00:55.936544\n",
      "episode: 147, score: -123.72971935349703\n",
      "elapse time of 147th episode is: 0:00:52.147949\n",
      "episode: 148, score: -555.125443572017\n",
      "elapse time of 148th episode is: 0:00:49.655969\n",
      "episode: 149, score: -133.53118675628406\n",
      "TargetNetwork equalized to DQNetworkPendulum at 149 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode149/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode149/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 149th episode is: 0:00:50.050171\n",
      "episode: 150, score: -263.85428477729965\n",
      "elapse time of 150th episode is: 0:00:49.698390\n",
      "episode: 151, score: -240.28648261561133\n",
      "elapse time of 151th episode is: 0:00:49.425639\n",
      "episode: 152, score: -314.22906958028136\n",
      "elapse time of 152th episode is: 0:00:49.352552\n",
      "episode: 153, score: -134.45716897371113\n",
      "elapse time of 153th episode is: 0:00:49.585019\n",
      "episode: 154, score: -257.24135374050906\n",
      "elapse time of 154th episode is: 0:00:49.750440\n",
      "episode: 155, score: -4.4720636621196554\n",
      "elapse time of 155th episode is: 0:00:51.101347\n",
      "episode: 156, score: -236.40474882083325\n",
      "elapse time of 156th episode is: 0:00:49.432337\n",
      "episode: 157, score: -130.75446835426916\n",
      "elapse time of 157th episode is: 0:00:58.674694\n",
      "episode: 158, score: -374.47448867493455\n",
      "elapse time of 158th episode is: 0:00:56.512313\n",
      "episode: 159, score: -247.09319111192457\n",
      "TargetNetwork equalized to DQNetworkPendulum at 159 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode159/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode159/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 159th episode is: 0:00:51.657344\n",
      "episode: 160, score: -127.72724711524016\n",
      "elapse time of 160th episode is: 0:00:49.903695\n",
      "episode: 161, score: -240.67521263591274\n",
      "elapse time of 161th episode is: 0:00:50.170131\n",
      "episode: 162, score: -2.598405642744004\n",
      "elapse time of 162th episode is: 0:00:49.966336\n",
      "episode: 163, score: -130.77713614841835\n",
      "elapse time of 163th episode is: 0:00:50.891567\n",
      "episode: 164, score: -265.2068144757872\n",
      "elapse time of 164th episode is: 0:00:51.148034\n",
      "episode: 165, score: -128.3944752247822\n",
      "elapse time of 165th episode is: 0:01:04.225006\n",
      "episode: 166, score: -253.65421362944622\n",
      "elapse time of 166th episode is: 0:00:51.211389\n",
      "episode: 167, score: -270.1972196608121\n",
      "elapse time of 167th episode is: 0:00:51.604931\n",
      "episode: 168, score: -130.394345162788\n",
      "elapse time of 168th episode is: 0:00:51.381833\n",
      "episode: 169, score: -135.4645844962397\n",
      "TargetNetwork equalized to DQNetworkPendulum at 169 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode169/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode169/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 169th episode is: 0:00:52.278020\n",
      "episode: 170, score: -4.14363495239638\n",
      "elapse time of 170th episode is: 0:00:54.295955\n",
      "episode: 171, score: -121.28716655686188\n",
      "elapse time of 171th episode is: 0:03:16.782898\n",
      "episode: 172, score: -372.80756810904523\n",
      "elapse time of 172th episode is: 0:01:01.296643\n",
      "episode: 173, score: -3.621204609254975\n",
      "elapse time of 173th episode is: 0:00:51.600688\n",
      "episode: 174, score: -259.4231643073962\n",
      "elapse time of 174th episode is: 0:00:50.927754\n",
      "episode: 175, score: -255.7801386044264\n",
      "elapse time of 175th episode is: 0:00:57.512734\n",
      "episode: 176, score: -238.09906222736421\n",
      "elapse time of 176th episode is: 0:00:54.262615\n",
      "episode: 177, score: -130.02095517665214\n",
      "elapse time of 177th episode is: 0:00:51.721402\n",
      "episode: 178, score: -274.57365745988403\n",
      "elapse time of 178th episode is: 0:00:52.357047\n",
      "episode: 179, score: -130.2837193671793\n",
      "TargetNetwork equalized to DQNetworkPendulum at 179 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode179/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode179/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 179th episode is: 0:00:53.311839\n",
      "episode: 180, score: -129.99311385083172\n",
      "elapse time of 180th episode is: 0:00:54.697983\n",
      "episode: 181, score: -247.48942487740504\n",
      "elapse time of 181th episode is: 0:00:57.584998\n",
      "episode: 182, score: -132.2008042210899\n",
      "elapse time of 182th episode is: 0:00:59.320713\n",
      "episode: 183, score: -128.03588013093602\n",
      "elapse time of 183th episode is: 0:00:52.811491\n",
      "episode: 184, score: -272.4831476462637\n",
      "elapse time of 184th episode is: 0:01:57.719796\n",
      "episode: 185, score: -125.25104985636213\n",
      "elapse time of 185th episode is: 0:00:49.113279\n",
      "episode: 186, score: -126.81226473896366\n",
      "elapse time of 186th episode is: 0:00:49.279828\n",
      "episode: 187, score: -238.3228607871768\n",
      "elapse time of 187th episode is: 0:00:50.161114\n",
      "episode: 188, score: -130.93241736370032\n",
      "elapse time of 188th episode is: 0:00:48.953556\n",
      "episode: 189, score: -256.86144307443715\n",
      "TargetNetwork equalized to DQNetworkPendulum at 189 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode189/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode189/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 189th episode is: 0:00:50.313816\n",
      "episode: 190, score: -524.9599317800256\n",
      "elapse time of 190th episode is: 0:00:49.650129\n",
      "episode: 191, score: -129.23166899206774\n",
      "elapse time of 191th episode is: 0:00:48.923218\n",
      "episode: 192, score: -397.4448449008524\n",
      "elapse time of 192th episode is: 0:00:48.650999\n",
      "episode: 193, score: -134.92802516692615\n",
      "elapse time of 193th episode is: 0:00:51.013551\n",
      "episode: 194, score: -132.53915987864193\n",
      "elapse time of 194th episode is: 0:00:49.621345\n",
      "episode: 195, score: -128.80779722450427\n",
      "elapse time of 195th episode is: 0:00:54.559838\n",
      "episode: 196, score: -126.56433382079732\n",
      "elapse time of 196th episode is: 0:00:51.568707\n",
      "episode: 197, score: -130.8286160196871\n",
      "elapse time of 197th episode is: 0:00:49.387062\n",
      "episode: 198, score: -373.4085622619808\n",
      "elapse time of 198th episode is: 0:00:49.046530\n",
      "episode: 199, score: -7.898844389032347\n",
      "TargetNetwork equalized to DQNetworkPendulum at 199 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode199/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode199/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 199th episode is: 0:00:51.590906\n",
      "episode: 200, score: -238.52331266881833\n",
      "elapse time of 200th episode is: 0:00:49.709178\n",
      "episode: 201, score: -390.97791806290957\n",
      "elapse time of 201th episode is: 0:00:50.290426\n",
      "episode: 202, score: -128.98510471919553\n",
      "elapse time of 202th episode is: 0:00:49.726261\n",
      "episode: 203, score: -388.9249119339776\n",
      "elapse time of 203th episode is: 0:00:49.777520\n",
      "episode: 204, score: -487.6749216080319\n",
      "elapse time of 204th episode is: 0:00:49.192646\n",
      "episode: 205, score: -256.45644713921655\n",
      "elapse time of 205th episode is: 0:00:50.253516\n",
      "episode: 206, score: -122.07448015391431\n",
      "elapse time of 206th episode is: 0:00:51.142676\n",
      "episode: 207, score: -395.85204743516175\n",
      "elapse time of 207th episode is: 0:00:52.067081\n",
      "episode: 208, score: -248.2530559649225\n",
      "elapse time of 208th episode is: 0:00:48.780901\n",
      "episode: 209, score: -256.3915007922867\n",
      "TargetNetwork equalized to DQNetworkPendulum at 209 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode209/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode209/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 209th episode is: 0:00:50.494851\n",
      "episode: 210, score: -132.8851263971423\n",
      "elapse time of 210th episode is: 0:00:49.296986\n",
      "episode: 211, score: -122.28200861618586\n",
      "elapse time of 211th episode is: 0:00:49.814475\n",
      "episode: 212, score: -248.2307507439616\n",
      "elapse time of 212th episode is: 0:00:50.583769\n",
      "episode: 213, score: -123.19206507069198\n",
      "elapse time of 213th episode is: 0:00:50.649698\n",
      "episode: 214, score: -129.1282728197807\n",
      "elapse time of 214th episode is: 0:00:49.726870\n",
      "episode: 215, score: -261.4834007689443\n",
      "elapse time of 215th episode is: 0:00:51.244399\n",
      "episode: 216, score: -8.510557126394904\n",
      "elapse time of 216th episode is: 0:00:50.571741\n",
      "episode: 217, score: -130.299573079232\n",
      "elapse time of 217th episode is: 0:00:50.694410\n",
      "episode: 218, score: -479.5197674565832\n",
      "elapse time of 218th episode is: 0:00:50.511973\n",
      "episode: 219, score: -256.976302011647\n",
      "TargetNetwork equalized to DQNetworkPendulum at 219 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode219/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode219/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 219th episode is: 0:00:51.274656\n",
      "episode: 220, score: -134.25994495866775\n",
      "elapse time of 220th episode is: 0:00:50.378196\n",
      "episode: 221, score: -259.51596308812793\n",
      "elapse time of 221th episode is: 0:00:51.173944\n",
      "episode: 222, score: -134.49084176289725\n",
      "elapse time of 222th episode is: 0:00:55.443698\n",
      "episode: 223, score: -128.20562386157172\n",
      "elapse time of 223th episode is: 0:00:52.096894\n",
      "episode: 224, score: -10.905926704310303\n",
      "elapse time of 224th episode is: 0:00:50.625924\n",
      "episode: 225, score: -126.1571686204477\n",
      "elapse time of 225th episode is: 0:00:50.206806\n",
      "episode: 226, score: -233.8323093743621\n",
      "elapse time of 226th episode is: 0:00:49.981695\n",
      "episode: 227, score: -367.4456416660678\n",
      "elapse time of 227th episode is: 0:00:50.186296\n",
      "episode: 228, score: -128.59202106817338\n",
      "elapse time of 228th episode is: 0:00:50.715696\n",
      "episode: 229, score: -387.8263645903435\n",
      "TargetNetwork equalized to DQNetworkPendulum at 229 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode229/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode229/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 229th episode is: 0:00:51.689698\n",
      "episode: 230, score: -133.65045323214565\n",
      "elapse time of 230th episode is: 0:00:50.508193\n",
      "episode: 231, score: -362.08282408349714\n",
      "elapse time of 231th episode is: 0:00:50.041892\n",
      "episode: 232, score: -133.6794725988595\n",
      "elapse time of 232th episode is: 0:00:54.269228\n",
      "episode: 233, score: -134.36555849382054\n",
      "elapse time of 233th episode is: 0:00:57.606095\n",
      "episode: 234, score: -132.94766033331263\n",
      "elapse time of 234th episode is: 0:00:50.473432\n",
      "episode: 235, score: -365.79132795318156\n",
      "elapse time of 235th episode is: 0:00:50.556033\n",
      "episode: 236, score: -128.23808687767152\n",
      "elapse time of 236th episode is: 0:00:48.891429\n",
      "episode: 237, score: -133.6735719099911\n",
      "elapse time of 237th episode is: 0:00:49.631371\n",
      "episode: 238, score: -248.2561338952395\n",
      "elapse time of 238th episode is: 0:00:51.187222\n",
      "episode: 239, score: -252.83489577610453\n",
      "TargetNetwork equalized to DQNetworkPendulum at 239 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode239/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode239/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 239th episode is: 0:00:51.037576\n",
      "episode: 240, score: -129.06913996471454\n",
      "elapse time of 240th episode is: 0:00:55.268164\n",
      "episode: 241, score: -129.36299726928576\n",
      "elapse time of 241th episode is: 0:00:58.186050\n",
      "episode: 242, score: -126.31718080593917\n",
      "elapse time of 242th episode is: 0:00:51.140010\n",
      "episode: 243, score: -8.256888397901902\n",
      "elapse time of 243th episode is: 0:00:49.932066\n",
      "episode: 244, score: -247.34137569308507\n",
      "elapse time of 244th episode is: 0:00:50.215925\n",
      "episode: 245, score: -252.68700165460606\n",
      "elapse time of 245th episode is: 0:00:50.099179\n",
      "episode: 246, score: -253.5759351972531\n",
      "elapse time of 246th episode is: 0:00:51.895527\n",
      "episode: 247, score: -8.861571041069961\n",
      "elapse time of 247th episode is: 0:00:50.570379\n",
      "episode: 248, score: -7.403225925877629\n",
      "elapse time of 248th episode is: 0:00:50.784270\n",
      "episode: 249, score: -122.93960601255775\n",
      "TargetNetwork equalized to DQNetworkPendulum at 249 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode249/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode249/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 249th episode is: 0:01:00.869621\n",
      "episode: 250, score: -352.76775484195014\n",
      "elapse time of 250th episode is: 0:03:08.417927\n",
      "episode: 251, score: -262.72876772415594\n",
      "elapse time of 251th episode is: 0:01:00.246328\n",
      "episode: 252, score: -366.8995339742792\n",
      "elapse time of 252th episode is: 0:00:59.487460\n",
      "episode: 253, score: -250.20444456315727\n",
      "elapse time of 253th episode is: 0:00:51.816593\n",
      "episode: 254, score: -258.4728725964347\n",
      "elapse time of 254th episode is: 0:00:51.842800\n",
      "episode: 255, score: -133.94170505521345\n",
      "elapse time of 255th episode is: 0:00:51.761897\n",
      "episode: 256, score: -205.92952786135191\n",
      "elapse time of 256th episode is: 0:00:52.385927\n",
      "episode: 257, score: -525.5124461806854\n",
      "elapse time of 257th episode is: 0:00:55.834201\n",
      "episode: 258, score: -127.0419101122107\n",
      "elapse time of 258th episode is: 0:01:00.174765\n",
      "episode: 259, score: -536.7830902546856\n",
      "TargetNetwork equalized to DQNetworkPendulum at 259 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode259/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode259/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 259th episode is: 0:00:52.648822\n",
      "episode: 260, score: -237.74858911190236\n",
      "elapse time of 260th episode is: 0:00:50.946538\n",
      "episode: 261, score: -135.35700417380042\n",
      "elapse time of 261th episode is: 0:00:50.809695\n",
      "episode: 262, score: -254.54923122111043\n",
      "elapse time of 262th episode is: 0:00:55.909519\n",
      "episode: 263, score: -132.54051357711015\n",
      "elapse time of 263th episode is: 0:00:53.094145\n",
      "episode: 264, score: -130.98493638348015\n",
      "elapse time of 264th episode is: 0:00:50.125051\n",
      "episode: 265, score: -251.9892965248824\n",
      "elapse time of 265th episode is: 0:00:52.088781\n",
      "episode: 266, score: -134.11425454534262\n",
      "elapse time of 266th episode is: 0:00:50.248474\n",
      "episode: 267, score: -133.79639021138698\n",
      "elapse time of 267th episode is: 0:00:56.728984\n",
      "episode: 268, score: -247.58884678659413\n",
      "elapse time of 268th episode is: 0:01:08.922911\n",
      "episode: 269, score: -358.47994023999416\n",
      "TargetNetwork equalized to DQNetworkPendulum at 269 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode269/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode269/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 269th episode is: 0:00:55.004730\n",
      "episode: 270, score: -134.36628193979308\n",
      "elapse time of 270th episode is: 0:00:51.843175\n",
      "episode: 271, score: -121.16554742041997\n",
      "elapse time of 271th episode is: 0:00:51.647255\n",
      "episode: 272, score: -794.2977188063265\n",
      "elapse time of 272th episode is: 0:00:51.230922\n",
      "episode: 273, score: -355.64877548257243\n",
      "elapse time of 273th episode is: 0:00:57.580691\n",
      "episode: 274, score: -329.69377744245133\n",
      "elapse time of 274th episode is: 0:01:01.368773\n",
      "episode: 275, score: -122.8594762904756\n",
      "elapse time of 275th episode is: 0:00:53.893211\n",
      "episode: 276, score: -129.1045661082969\n",
      "elapse time of 276th episode is: 0:00:51.942692\n",
      "episode: 277, score: -126.6536017332261\n",
      "elapse time of 277th episode is: 0:00:50.856912\n",
      "episode: 278, score: -248.63937069153002\n",
      "elapse time of 278th episode is: 0:00:52.242244\n",
      "episode: 279, score: -228.22375079909222\n",
      "TargetNetwork equalized to DQNetworkPendulum at 279 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode279/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode279/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 279th episode is: 0:00:51.785925\n",
      "episode: 280, score: -395.72307744674083\n",
      "elapse time of 280th episode is: 0:01:00.678662\n",
      "episode: 281, score: -131.21879790944664\n",
      "elapse time of 281th episode is: 0:00:54.346273\n",
      "episode: 282, score: -265.2877255159844\n",
      "elapse time of 282th episode is: 0:00:50.904179\n",
      "episode: 283, score: -530.5666193459748\n",
      "elapse time of 283th episode is: 0:00:51.616865\n",
      "episode: 284, score: -247.9970964231507\n",
      "elapse time of 284th episode is: 0:00:56.248200\n",
      "episode: 285, score: -240.86724619064677\n",
      "elapse time of 285th episode is: 0:00:54.869779\n",
      "episode: 286, score: -126.18670535913508\n",
      "elapse time of 286th episode is: 0:01:04.464892\n",
      "episode: 287, score: -4.48456313442338\n",
      "elapse time of 287th episode is: 0:00:52.509487\n",
      "episode: 288, score: -7.540232548713467\n",
      "elapse time of 288th episode is: 0:00:52.397454\n",
      "episode: 289, score: -463.87805922054696\n",
      "TargetNetwork equalized to DQNetworkPendulum at 289 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode289/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode289/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 289th episode is: 0:00:52.193054\n",
      "episode: 290, score: -119.85366362413974\n",
      "elapse time of 290th episode is: 0:00:57.080409\n",
      "episode: 291, score: -382.58121525178336\n",
      "elapse time of 291th episode is: 0:01:03.496074\n",
      "episode: 292, score: -247.19256176867248\n",
      "elapse time of 292th episode is: 0:00:55.310794\n",
      "episode: 293, score: -122.75195737112917\n",
      "elapse time of 293th episode is: 0:00:51.525882\n",
      "episode: 294, score: -535.5969121248885\n",
      "elapse time of 294th episode is: 0:00:52.063264\n",
      "episode: 295, score: -699.7593860271111\n",
      "elapse time of 295th episode is: 0:00:51.852216\n",
      "episode: 296, score: -131.73579086851797\n",
      "elapse time of 296th episode is: 0:00:56.102182\n",
      "episode: 297, score: -266.54834699866285\n",
      "elapse time of 297th episode is: 0:01:05.879562\n",
      "episode: 298, score: -126.00742220844471\n",
      "elapse time of 298th episode is: 0:00:54.586042\n",
      "episode: 299, score: -382.7425559653554\n",
      "TargetNetwork equalized to DQNetworkPendulum at 299 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode299/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode299/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 299th episode is: 0:00:53.537620\n",
      "episode: 300, score: -253.21484137889362\n",
      "elapse time of 300th episode is: 0:00:54.490984\n",
      "episode: 301, score: -140.55044413219636\n",
      "elapse time of 301th episode is: 0:00:59.173529\n",
      "episode: 302, score: -135.93454635099783\n",
      "elapse time of 302th episode is: 0:01:04.628680\n",
      "episode: 303, score: -353.6733760153164\n",
      "elapse time of 303th episode is: 0:00:53.281006\n",
      "episode: 304, score: -614.8115926070166\n",
      "elapse time of 304th episode is: 0:00:52.646393\n",
      "episode: 305, score: -120.64261352564257\n",
      "elapse time of 305th episode is: 0:00:57.714406\n",
      "episode: 306, score: -126.46959060934778\n",
      "elapse time of 306th episode is: 0:01:03.583801\n",
      "episode: 307, score: -136.82206204463571\n",
      "elapse time of 307th episode is: 0:00:55.751562\n",
      "episode: 308, score: -131.5523886371371\n",
      "elapse time of 308th episode is: 0:00:52.936635\n",
      "episode: 309, score: -511.101154701172\n",
      "TargetNetwork equalized to DQNetworkPendulum at 309 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode309/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode309/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 309th episode is: 0:00:59.145133\n",
      "episode: 310, score: -7.111995367928022\n",
      "elapse time of 310th episode is: 0:01:03.153596\n",
      "episode: 311, score: -122.69508070307134\n",
      "elapse time of 311th episode is: 0:00:57.669667\n",
      "episode: 312, score: -385.82701562789555\n",
      "elapse time of 312th episode is: 0:00:53.648987\n",
      "episode: 313, score: -8.294496830717822\n",
      "elapse time of 313th episode is: 0:00:57.392058\n",
      "episode: 314, score: -128.93959231700742\n",
      "elapse time of 314th episode is: 0:01:03.043027\n",
      "episode: 315, score: -130.28444480558235\n",
      "elapse time of 315th episode is: 0:01:02.275242\n",
      "episode: 316, score: -359.81479213002774\n",
      "elapse time of 316th episode is: 0:00:54.051271\n",
      "episode: 317, score: -128.64196809453736\n",
      "elapse time of 317th episode is: 0:00:58.672334\n",
      "episode: 318, score: -751.5812054150451\n",
      "elapse time of 318th episode is: 0:01:03.128684\n",
      "episode: 319, score: -132.99296397012088\n",
      "TargetNetwork equalized to DQNetworkPendulum at 319 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode319/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode319/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 319th episode is: 0:01:03.435310\n",
      "episode: 320, score: -233.13467461249073\n",
      "elapse time of 320th episode is: 0:00:54.229162\n",
      "episode: 321, score: -414.0347143246368\n",
      "elapse time of 321th episode is: 0:00:56.921099\n",
      "episode: 322, score: -257.20059541153137\n",
      "elapse time of 322th episode is: 0:01:06.666595\n",
      "episode: 323, score: -247.49677444689829\n",
      "elapse time of 323th episode is: 0:00:57.092146\n",
      "episode: 324, score: -122.31652378401836\n",
      "elapse time of 324th episode is: 0:00:53.879278\n",
      "episode: 325, score: -509.9037264966122\n",
      "elapse time of 325th episode is: 0:00:58.583018\n",
      "episode: 326, score: -1046.2062625189587\n",
      "elapse time of 326th episode is: 0:01:06.988228\n",
      "episode: 327, score: -384.5944596764266\n",
      "elapse time of 327th episode is: 0:00:56.598564\n",
      "episode: 328, score: -1064.28033845974\n",
      "elapse time of 328th episode is: 0:00:57.640278\n",
      "episode: 329, score: -126.91055462405626\n",
      "TargetNetwork equalized to DQNetworkPendulum at 329 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode329/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode329/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 329th episode is: 0:01:02.381422\n",
      "episode: 330, score: -900.8548502254218\n",
      "elapse time of 330th episode is: 0:01:02.998456\n",
      "episode: 331, score: -2.0775010997603167\n",
      "elapse time of 331th episode is: 0:00:54.003191\n",
      "episode: 332, score: -122.26467970424375\n",
      "elapse time of 332th episode is: 0:00:58.003604\n",
      "episode: 333, score: -123.82894588134144\n",
      "elapse time of 333th episode is: 0:01:07.690709\n",
      "episode: 334, score: -124.48915049758844\n",
      "elapse time of 334th episode is: 0:00:58.321434\n",
      "episode: 335, score: -274.64119276866086\n",
      "elapse time of 335th episode is: 0:00:54.948888\n",
      "episode: 336, score: -122.00748385728292\n",
      "elapse time of 336th episode is: 0:01:00.559404\n",
      "episode: 337, score: -117.19563133706028\n",
      "elapse time of 337th episode is: 0:01:05.758660\n",
      "episode: 338, score: -1.5871473167324914\n",
      "elapse time of 338th episode is: 0:00:55.540003\n",
      "episode: 339, score: -122.35063692501289\n",
      "TargetNetwork equalized to DQNetworkPendulum at 339 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode339/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode339/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 339th episode is: 0:00:57.795944\n",
      "episode: 340, score: -121.8538784196621\n",
      "elapse time of 340th episode is: 0:01:02.711023\n",
      "episode: 341, score: -119.29749475660348\n",
      "elapse time of 341th episode is: 0:01:02.965472\n",
      "episode: 342, score: -254.53766352936134\n",
      "elapse time of 342th episode is: 0:00:57.795866\n",
      "episode: 343, score: -348.7377475712828\n",
      "elapse time of 343th episode is: 0:01:00.773754\n",
      "episode: 344, score: -121.10839663538232\n",
      "elapse time of 344th episode is: 0:01:04.685748\n",
      "episode: 345, score: -118.83094815864249\n",
      "elapse time of 345th episode is: 0:00:55.385005\n",
      "episode: 346, score: -127.89900126574653\n",
      "elapse time of 346th episode is: 0:00:59.092294\n",
      "episode: 347, score: -123.35554410358218\n",
      "elapse time of 347th episode is: 0:01:05.114814\n",
      "episode: 348, score: -1.7998507915417716\n",
      "elapse time of 348th episode is: 0:00:59.183035\n",
      "episode: 349, score: -121.31960658454001\n",
      "TargetNetwork equalized to DQNetworkPendulum at 349 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode349/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode349/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 349th episode is: 0:00:59.930501\n",
      "episode: 350, score: -122.25821452151523\n",
      "elapse time of 350th episode is: 0:00:59.768533\n",
      "episode: 351, score: -365.7629103734631\n",
      "elapse time of 351th episode is: 0:01:05.732131\n",
      "episode: 352, score: -231.79696500806517\n",
      "elapse time of 352th episode is: 0:00:55.113353\n",
      "episode: 353, score: -123.25078881543392\n",
      "elapse time of 353th episode is: 0:01:01.830007\n",
      "episode: 354, score: -4.109529211552181\n",
      "elapse time of 354th episode is: 0:01:06.729349\n",
      "episode: 355, score: -402.5442206675482\n",
      "elapse time of 355th episode is: 0:00:55.718803\n",
      "episode: 356, score: -234.62796576391918\n",
      "elapse time of 356th episode is: 0:00:57.638570\n",
      "episode: 357, score: -368.6438119407183\n",
      "elapse time of 357th episode is: 0:07:17.657625\n",
      "episode: 358, score: -126.14535862659707\n",
      "elapse time of 358th episode is: 0:00:54.116233\n",
      "episode: 359, score: -350.8065366297496\n",
      "TargetNetwork equalized to DQNetworkPendulum at 359 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode359/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode359/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 359th episode is: 0:00:50.242197\n",
      "episode: 360, score: -513.6530671320967\n",
      "elapse time of 360th episode is: 0:00:52.450738\n",
      "episode: 361, score: -363.61709812726355\n",
      "elapse time of 361th episode is: 0:00:49.742511\n",
      "episode: 362, score: -3.3353082330018524\n",
      "elapse time of 362th episode is: 0:00:50.123309\n",
      "episode: 363, score: -355.1703827401887\n",
      "elapse time of 363th episode is: 0:00:50.477351\n",
      "episode: 364, score: -126.92336142165547\n",
      "elapse time of 364th episode is: 0:00:50.969405\n",
      "episode: 365, score: -127.3452589462304\n",
      "elapse time of 365th episode is: 0:00:50.816840\n",
      "episode: 366, score: -124.51115814757617\n",
      "elapse time of 366th episode is: 0:00:51.454648\n",
      "episode: 367, score: -362.54195162582243\n",
      "elapse time of 367th episode is: 0:00:58.793271\n",
      "episode: 368, score: -125.54291501389702\n",
      "elapse time of 368th episode is: 0:00:54.392554\n",
      "episode: 369, score: -126.1669207182617\n",
      "TargetNetwork equalized to DQNetworkPendulum at 369 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode369/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode369/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 369th episode is: 0:00:52.199514\n",
      "episode: 370, score: -132.22775897510897\n",
      "elapse time of 370th episode is: 0:00:57.668498\n",
      "episode: 371, score: -281.48048790497205\n",
      "elapse time of 371th episode is: 0:00:51.286470\n",
      "episode: 372, score: -5.682722094244195\n",
      "elapse time of 372th episode is: 0:00:49.909295\n",
      "episode: 373, score: -237.12748527974864\n",
      "elapse time of 373th episode is: 0:00:50.226643\n",
      "episode: 374, score: -238.95131106451677\n",
      "elapse time of 374th episode is: 0:00:49.532924\n",
      "episode: 375, score: -240.68946622222518\n",
      "elapse time of 375th episode is: 0:00:51.455226\n",
      "episode: 376, score: -128.55953026088574\n",
      "elapse time of 376th episode is: 0:00:54.792745\n",
      "episode: 377, score: -247.02130072778985\n",
      "elapse time of 377th episode is: 0:00:50.290376\n",
      "episode: 378, score: -256.0918013593865\n",
      "elapse time of 378th episode is: 0:00:49.575730\n",
      "episode: 379, score: -124.71134800420681\n",
      "TargetNetwork equalized to DQNetworkPendulum at 379 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode379/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode379/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 379th episode is: 0:00:51.234606\n",
      "episode: 380, score: -363.61284961090803\n",
      "elapse time of 380th episode is: 0:00:50.276277\n",
      "episode: 381, score: -234.15321914855866\n",
      "elapse time of 381th episode is: 0:00:51.827318\n",
      "episode: 382, score: -122.63458624381225\n",
      "elapse time of 382th episode is: 0:00:52.303272\n",
      "episode: 383, score: -126.90146498433843\n",
      "elapse time of 383th episode is: 0:00:53.583396\n",
      "episode: 384, score: -124.96737839039955\n",
      "elapse time of 384th episode is: 0:01:16.263480\n",
      "episode: 385, score: -123.8557156683999\n",
      "elapse time of 385th episode is: 0:00:53.592533\n",
      "episode: 386, score: -123.24618054221936\n",
      "elapse time of 386th episode is: 0:00:57.474369\n",
      "episode: 387, score: -5.189924731058778\n",
      "elapse time of 387th episode is: 0:01:05.434938\n",
      "episode: 388, score: -478.184819641698\n",
      "elapse time of 388th episode is: 0:00:58.743767\n",
      "episode: 389, score: -125.51824060933077\n",
      "TargetNetwork equalized to DQNetworkPendulum at 389 th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode389/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DQNetworkPendulum_episode389/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time of 389th episode is: 0:01:02.240504\n",
      "episode: 390, score: -237.45577865784102\n",
      "elapse time of 390th episode is: 0:01:01.132697\n",
      "episode: 391, score: -128.01984320802208\n",
      "elapse time of 391th episode is: 0:00:53.287068\n",
      "episode: 392, score: -123.1610251438562\n",
      "elapse time of 392th episode is: 0:00:51.738245\n",
      "episode: 393, score: -232.6444312790808\n",
      "elapse time of 393th episode is: 0:00:50.147370\n",
      "episode: 394, score: -243.9123624507026\n",
      "elapse time of 394th episode is: 0:00:51.135541\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.97\n",
    "n_episodes = 500\n",
    "episode_scores = []\n",
    "\n",
    "for episode in range(0, n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state , [1,state_size])\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "    best_score = 0\n",
    "    epsilon = 0.97 ** episode\n",
    "    \n",
    "    start_time_episode = datetime.now()\n",
    "    while not done:\n",
    "        action, action_index = epsilon_greedy(DQNetworkPendulum, state, eps = epsilon)\n",
    "        #time.sleep(0.05)\n",
    "        #env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state , [1,state_size])\n",
    "        \n",
    "        #if done and episode_score != (-env._max_episode_steps)+1: # 200. adımda done olduysa yani artık oyun bitti hedefe ulaşamadı o zman -100 ceza ver\n",
    "        #    reward = 200\n",
    "        #else: # episodun sonuna gelip done da TRUE\n",
    "        #    reward = reward\n",
    "            \n",
    "        memory.append((state, action_index, reward, next_state, done))\n",
    "        \n",
    "        if len(memory) > min_experience:\n",
    "            minibatch = random.sample(memory, min(len(memory), batch_size ))\n",
    "            states = np.zeros((batch_size, state_size))\n",
    "            next_states = np.zeros((batch_size, state_size))\n",
    "            \n",
    "            actions, rewards, dones = [], [], []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                states[i] = minibatch[i][0]\n",
    "                actions.append((minibatch[i][1]))\n",
    "                rewards.append((minibatch[i][2]))\n",
    "                next_states[i] = minibatch[i][3]\n",
    "                dones.append((minibatch[i][4]))\n",
    "                \n",
    "            targets = DQNetworkPendulum.predict(states, verbose = 0)\n",
    "            target_nexts = TargetNetworkPendulum.predict(next_states, verbose = 0)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    targets[i][actions[i]] = rewards[i] + GAMMA * (np.amax(target_nexts[i]))\n",
    "            \n",
    "            \n",
    "            DQNetworkPendulum.fit(states, targets, batch_size = batch_size, verbose = 0)\n",
    "        state = next_state\n",
    "        \n",
    "        episode_score += reward\n",
    "        \n",
    "        if done:\n",
    "            print(\"episode: {}, score: {}\".format(episode, episode_score))\n",
    "            \n",
    "    episode_scores.append(episode_score)\n",
    "    np.savetxt(\"Log/Episode_scores.txt\",episode_scores)\n",
    "    if (episode + 1)%10 == 0:\n",
    "        TargetNetworkPendulum.set_weights(DQNetworkPendulum.get_weights())\n",
    "        print(\"TargetNetwork equalized to DQNetworkPendulum at {} th episode\".format(episode))\n",
    "        name = \"DQNetworkPendulum_episode{}\".format(episode)\n",
    "        DQNetworkPendulum.save(name)\n",
    "    end_time_episode = datetime.now()\n",
    "    print(\"elapse time of {}th episode is: {}\".format(episode,(end_time_episode - start_time_episode)))\n",
    "        \n",
    "plt.plot(episode_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f555bf7",
   "metadata": {},
   "source": [
    "# Watch Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b005cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [-2. , -1.6, -1.2, -0.8, -0.4,  0. ,  0.4,  0.8,  1.2,  1.6,  2. ]\n",
    "def epsilon_greedy(model, state, eps):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - max(eps, 0.05)): # Burada mutlaka keşfetmeyi sağlama amacıyla yapıldı\n",
    "        max_index = np.argmax(model.predict(state,verbose=0))\n",
    "        return np.array([action_space[max_index]]), max_index\n",
    "    else:\n",
    "        random_index = np.random.choice(np.arange(len(action_space)))\n",
    "        return np.array([action_space[random_index]]), random_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3f6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "\n",
    "state_size = len(env.observation_space.sample())\n",
    "action_size = env.action_space.sample().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eccd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_agent(model):\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    s = env.reset()\n",
    "    s = np.reshape(s,[1,state_size])\n",
    "    while not done:\n",
    "        a, action_index = epsilon_greedy(DQNetworkPendulum, s, eps = 0)\n",
    "        s,r,done,info=env.step(a)\n",
    "        s = np.reshape(s,[1,state_size])\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "        episode_reward +=r\n",
    "    print(\"Episode reward: \",episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db605353",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNetworkPendulum = tf.keras.models.load_model(\"DQNetworkPendulum_episode389\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c292f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward:  -129.49336723612825\n",
      "Episode reward:  -456.28802666783633\n",
      "Episode reward:  -1.6950611242237885\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    watch_agent(DQNetworkPendulum)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eeddd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0dc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe3d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "import gym\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNModel(input_size, action_size, hidden_layer_sizes, act, model_name):\n",
    "    model = Sequential(name = model_name)\n",
    "    model.add(Dense(hidden_layer_sizes[0], input_shape = (input_size,), activation = act, \n",
    "                    kernel_initializer='he_uniform'))\n",
    "    \n",
    "    if len(hidden_layer_sizes)>1:\n",
    "        for node_size in hidden_layer_sizes[1:]:\n",
    "            model.add(Dense(node_size, activation = act,kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(action_size, activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.compile(loss='mse', optimizer = RMSprop(learning_rate = 0.00025,epsilon = 0.01))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(model, state, eps):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - max(eps, 0.05)): # Burada mutlaka keşfetmeyi sağlama amacıyla yapıldı\n",
    "        return np.argmax(model.predict(state,verbose=0))\n",
    "    else:\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ddfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_agent(model):\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    s = env.reset()\n",
    "    s = np.reshape(s,[1,state_size])\n",
    "    while not done:\n",
    "        a = epsilon_greedy(model,s,eps=0)\n",
    "        s,r,done,info=env.step(a)\n",
    "        s = np.reshape(s,[1,state_size])\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "        episode_reward -=r\n",
    "    print(\"Episode reward: \",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bacc41",
   "metadata": {},
   "source": [
    "# Deep Q Network with BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ef0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=2000) \n",
    "min_experience = 500\n",
    "batch_size = 64\n",
    "np.random.seed(124)\n",
    "\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env._max_episode_steps = 1000\n",
    "\n",
    "state_size = len(env.observation_space.sample())\n",
    "action_size = env.action_space.n\n",
    "\n",
    "\n",
    "DQNModel =  NNModel(input_size = state_size,\n",
    "                   action_size = action_size, \n",
    "                   hidden_layer_sizes = [600,500, 200],\n",
    "                   act='tanh',model_name = 'MountainCar')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GAMMA = 0.95\n",
    "n_episodes = 5000\n",
    "episode_scores = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state , [1,state_size])\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "    best_score = 0\n",
    "    epsilon = 0.97 ** episode\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy(DQNModel, state, eps = epsilon)\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state , [1,state_size])\n",
    "        \n",
    "        if done and episode_score != (-env._max_episode_steps)+1: # 200. adımda done olduysa yani artık oyun bitti hedefe ulaşamadı o zman -100 ceza ver\n",
    "            reward = 100\n",
    "        else: # episodun sonuna gelip done da TRUE\n",
    "            reward = reward\n",
    "            \n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(memory) > min_experience:\n",
    "            minibatch = random.sample(memory, min(len(memory), batch_size ))\n",
    "            states = np.zeros((batch_size, state_size))\n",
    "            next_states = np.zeros((batch_size, state_size))\n",
    "            \n",
    "            actions, rewards, dones = [], [], []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                states[i] = minibatch[i][0]\n",
    "                actions.append((minibatch[i][1]))\n",
    "                rewards.append((minibatch[i][2]))\n",
    "                next_states[i] = minibatch[i][3]\n",
    "                dones.append((minibatch[i][4]))\n",
    "                \n",
    "            targets = DQNModel.predict(states,verbose=0)\n",
    "            target_nexts = DQNModel.predict(next_states,verbose=0)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    targets[i][actions[i]] = rewards[i] + GAMMA* (np.amax(target_nexts[i]))\n",
    "            \n",
    "            \n",
    "            DQNModel.fit(states, targets, batch_size = batch_size, verbose=0)\n",
    "        state = next_state\n",
    "        \n",
    "        episode_score = episode_score-1\n",
    "        if done:\n",
    "            print(\"episode: {}, score: {}\".format(episode, episode_score))\n",
    "            \n",
    "        episode_scores.append(episode_score)\n",
    "plt.plot(episode_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNModel.save(\"DQNCarBatchNoTarget\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"DQNCarBatchNoTarget\")\n",
    "\n",
    "for i in range(3):\n",
    "    watch_agent(model)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cfdcbc",
   "metadata": {},
   "source": [
    "# Deep Q Network with BATCH and Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"DQNmodel_episode19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1208bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=20000) \n",
    "min_experience = 2000\n",
    "batch_size = 128\n",
    "np.random.seed(124)\n",
    "\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env._max_episode_steps = 400\n",
    "\n",
    "state_size = len(env.observation_space.sample())\n",
    "action_size = env.action_space.n\n",
    "\n",
    "\n",
    "#DQNModel =  NNModel(input_size = state_size,\n",
    "#                    action_size = action_size, \n",
    "#                    hidden_layer_sizes = [200, 200, 200],\n",
    "#                    act='relu',\n",
    "#                    model_name = 'MountainCar')\n",
    "\n",
    "DQNModel = tf.keras.models.load_model(\"DQNmodel_episode49\")\n",
    "\n",
    "TargetNetwork =  NNModel(input_size = state_size,\n",
    "                         action_size = action_size, \n",
    "                         hidden_layer_sizes = [200, 200, 200],\n",
    "                         act='relu',\n",
    "                         model_name = 'MountainCar')\n",
    "\n",
    "TargetNetwork.set_weights(DQNModel.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eab26d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "n_episodes = 500\n",
    "episode_scores = []\n",
    "\n",
    "for episode in range(49, n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state , [1,state_size])\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "    best_score = 0\n",
    "    epsilon = 0.97 ** episode\n",
    "    \n",
    "    start_time_episode = datetime.now()\n",
    "    while not done:\n",
    "        action = epsilon_greedy(DQNModel, state, eps = epsilon)\n",
    "        #time.sleep(0.05)\n",
    "        #env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state , [1,state_size])\n",
    "        \n",
    "        if done and episode_score != (-env._max_episode_steps)+1: # 200. adımda done olduysa yani artık oyun bitti hedefe ulaşamadı o zman -100 ceza ver\n",
    "            reward = 200\n",
    "        else: # episodun sonuna gelip done da TRUE\n",
    "            reward = reward\n",
    "            \n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(memory) > min_experience:\n",
    "            minibatch = random.sample(memory, min(len(memory), batch_size ))\n",
    "            states = np.zeros((batch_size, state_size))\n",
    "            next_states = np.zeros((batch_size, state_size))\n",
    "            \n",
    "            actions, rewards, dones = [], [], []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                states[i] = minibatch[i][0]\n",
    "                actions.append((minibatch[i][1]))\n",
    "                rewards.append((minibatch[i][2]))\n",
    "                next_states[i] = minibatch[i][3]\n",
    "                dones.append((minibatch[i][4]))\n",
    "                \n",
    "            targets = DQNModel.predict(states, verbose = 0)\n",
    "            target_nexts = TargetNetwork.predict(next_states, verbose = 0)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    targets[i][actions[i]] = rewards[i] + GAMMA * (np.amax(target_nexts[i]))\n",
    "            \n",
    "            \n",
    "            DQNModel.fit(states, targets, batch_size = batch_size, verbose = 0)\n",
    "        state = next_state\n",
    "        \n",
    "        episode_score = episode_score-1\n",
    "        \n",
    "        if done:\n",
    "            print(\"episode: {}, score: {}\".format(episode, episode_score))\n",
    "            \n",
    "    episode_scores.append(episode_score)\n",
    "    if (episode + 1)%50 == 0:\n",
    "        TargetNetwork.set_weights(DQNModel.get_weights())\n",
    "        print(\"TargetNetwork equalized to DQNModel at {} th episode\".format(episode))\n",
    "        name = \"DQNmodel_episode{}\".format(episode)\n",
    "        DQNModel.save(name)\n",
    "    end_time_episode = datetime.now()\n",
    "    print(\"elapse time of {}th episode is: {}\".format(episode,(end_time_episode - start_time_episode)))\n",
    "        \n",
    "plt.plot(episode_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9a782",
   "metadata": {},
   "source": [
    "# Soft Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcec9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=20000) \n",
    "min_experience = 2000\n",
    "batch_size = 128\n",
    "np.random.seed(124)\n",
    "\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env._max_episode_steps = 400\n",
    "\n",
    "state_size = len(env.observation_space.sample())\n",
    "action_size = env.action_space.n\n",
    "\n",
    "\n",
    "DQNModel =  NNModel(input_size = state_size,\n",
    "                    action_size = action_size, \n",
    "                    hidden_layer_sizes = [200, 200, 200],\n",
    "                    act='tanh',\n",
    "                    model_name = 'MountainCar')\n",
    "\n",
    "TargetNetwork =  NNModel(input_size = state_size,\n",
    "                         action_size = action_size, \n",
    "                         hidden_layer_sizes = [200, 200, 200],\n",
    "                         act='tanh',\n",
    "                         model_name = 'MountainCar')\n",
    "\n",
    "TargetNetwork.set_weights(DQNModel.get_weights())\n",
    "\n",
    "TAU = 0.05\n",
    "\n",
    "\n",
    "\n",
    "GAMMA = 0.95\n",
    "n_episodes = 500\n",
    "episode_scores = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state , [1,state_size])\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "    best_score = 0\n",
    "    epsilon = 0.97 ** episode\n",
    "    \n",
    "    start_time_episode = datetime.now()\n",
    "    while not done:\n",
    "        action = epsilon_greedy(DQNModel, state, eps = epsilon)\n",
    "        #time.sleep(0.05)\n",
    "        #env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state , [1,state_size])\n",
    "        \n",
    "        if done and episode_score != (-env._max_episode_steps)+1: # 200. adımda done olduysa yani artık oyun bitti hedefe ulaşamadı o zman -100 ceza ver\n",
    "            reward = 100\n",
    "        else: # episodun sonuna gelip done da TRUE\n",
    "            reward = reward\n",
    "            \n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(memory) > min_experience:\n",
    "            minibatch = random.sample(memory, min(len(memory), batch_size ))\n",
    "            states = np.zeros((batch_size, state_size))\n",
    "            next_states = np.zeros((batch_size, state_size))\n",
    "            \n",
    "            actions, rewards, dones = [], [], []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                states[i] = minibatch[i][0]\n",
    "                actions.append((minibatch[i][1]))\n",
    "                rewards.append((minibatch[i][2]))\n",
    "                next_states[i] = minibatch[i][3]\n",
    "                dones.append((minibatch[i][4]))\n",
    "                \n",
    "            targets = DQNModel.predict(states, verbose = 0)\n",
    "            target_nexts = TargetNetwork.predict(next_states, verbose = 0)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    targets[i][actions[i]] = rewards[i] + GAMMA* (np.amax(target_nexts[i]))\n",
    "            \n",
    "            \n",
    "            DQNModel.fit(states, targets, batch_size = batch_size, verbose=0)\n",
    "            \n",
    "            # Soft Update\n",
    "            q_model_theta = DQNModel.get_weights()\n",
    "            target_model_theta = TargetNetwork.get_weights()\n",
    "            \n",
    "            \n",
    "            w_counter = 0\n",
    "            for q_weight, target_weight in zip(q_model_theta, target_model_theta):\n",
    "                target_weight = target_weight * (1-TAU) + q_weight*TAU\n",
    "                target_model_theta[w_counter] = target_weight\n",
    "                w_counter +=1\n",
    "            \n",
    "            TargetNetwork.set_weights(target_model_theta)\n",
    "        state = next_state\n",
    "        episode_score = episode_score-1\n",
    "        if done:\n",
    "            print(\"episode: {}, score: {}\".format(episode, episode_score))     \n",
    "    episode_scores.append(episode_score)\n",
    "    if (episode + 1)%50 == 0:\n",
    "        name = \"SoftUpdate/DQNmodelSoftUpdate_episode{}\".format(episode)\n",
    "        DQNModel.save(name)\n",
    "    end_time_episode = datetime.now()\n",
    "    print(\"elapse time of {}th episode is: {}\".format(episode,(end_time_episode - start_time_episode)))\n",
    "        \n",
    "plt.plot(episode_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb0be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
